<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>浅谈PCA中的Covariance matrix - Meixiuxiu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1.向量乘积的几何意义每次都要忘记这些东西所以记下来== 对于向量 a 和 b ,向量乘积定义为: 向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积 描述了向量间的相关性 2.PCA中的协方差矩阵2.1.PCA算法PCA算法是一种数据降维的算法，通过某种线性投影将高维的数据映射到低维的空间中，以此用比较少的数据维度保留住原数据的特性。 假设令A为m*n的原数据 ，m为样">
<meta name="keywords" content="PCA,Linear algebra">
<meta property="og:type" content="article">
<meta property="og:title" content="浅谈PCA中的Covariance matrix">
<meta property="og:url" content="http://yoursite.com/2017/10/29/Covariance matrix/index.html">
<meta property="og:site_name" content="Meixiuxiu">
<meta property="og:description" content="1.向量乘积的几何意义每次都要忘记这些东西所以记下来== 对于向量 a 和 b ,向量乘积定义为: 向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积 描述了向量间的相关性 2.PCA中的协方差矩阵2.1.PCA算法PCA算法是一种数据降维的算法，通过某种线性投影将高维的数据映射到低维的空间中，以此用比较少的数据维度保留住原数据的特性。 假设令A为m*n的原数据 ，m为样">
<meta property="og:locale" content="zh-tw">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/楼下的猫猫.JPG">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/27A466BF-B714-4F71-9718-D892C69191CA.png">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/AB84F733-5881-4B2C-8FE7-BE2099965DA2.png">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/0019B688-8129-4EF2-B87A-3ABAF7228268.png">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/66350F53-03C4-475F-B4B6-11E97AE8F471.png">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/F9CF233E-F1FA-421E-AB0F-612252E3A906.png">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/5583E423-CA73-43B5-AA42-E5F9A1967EC9.png">
<meta property="og:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/9A78E778-4C01-4AE1-9411-E461D3DB9E39.png">
<meta property="og:updated_time" content="2017-11-16T13:15:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="浅谈PCA中的Covariance matrix">
<meta name="twitter:description" content="1.向量乘积的几何意义每次都要忘记这些东西所以记下来== 对于向量 a 和 b ,向量乘积定义为: 向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积 描述了向量间的相关性 2.PCA中的协方差矩阵2.1.PCA算法PCA算法是一种数据降维的算法，通过某种线性投影将高维的数据映射到低维的空间中，以此用比较少的数据维度保留住原数据的特性。 假设令A为m*n的原数据 ，m为样">
<meta name="twitter:image" content="http://yoursite.com/2017/10/29/Covariance%20matrix/楼下的猫猫.JPG">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

  <script src="https://leancloud.cn/scripts/lib/av-0.4.6.min.js"></script>
  <script>AV.initialize("ImAHEPDUTcPakLfndBrcOoKD-gzGzoHsz", "u9JMhMDd0UbR4ob7JBJ2jEQF");</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer"><article id="post-Covariance matrix" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      浅谈PCA中的Covariance matrix
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/10/29/Covariance matrix/" class="article-date">
  <time datetime="2017-10-29T10:46:01.000Z" itemprop="datePublished">2017-10-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a><span>></span><a class="article-category-link" href="/categories/machine-learning/斯坦福机器学习公开课/">斯坦福机器学习公开课</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><img src="/2017/10/29/Covariance matrix/楼下的猫猫.JPG" alt="楼下的猫猫"></p>
<h3 id="1-向量乘积的几何意义"><a href="#1-向量乘积的几何意义" class="headerlink" title="1.向量乘积的几何意义"></a>1.向量乘积的几何意义</h3><p>每次都要忘记这些东西所以记下来==</p>
<p>对于向量 <strong>a</strong> 和 <strong>b</strong> ,向量乘积定义为:<img src="/2017/10/29/Covariance matrix/27A466BF-B714-4F71-9718-D892C69191CA.png" alt=""></p>
<p>向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积</p>
<p>描述了向量间的相关性</p>
<h3 id="2-PCA中的协方差矩阵"><a href="#2-PCA中的协方差矩阵" class="headerlink" title="2.PCA中的协方差矩阵"></a>2.PCA中的协方差矩阵</h3><h4 id="2-1-PCA算法"><a href="#2-1-PCA算法" class="headerlink" title="2.1.PCA算法"></a>2.1.PCA算法</h4><p>PCA算法是一种数据降维的算法，通过某种线性投影将高维的数据映射到低维的空间中，以此用比较少的数据维度保留住原数据的特性。</p>
<p>假设令A为m*n的原数据 ，m为样本数量，n为维度</p>
<ul>
<li>对每个维度的数据进行中心化</li>
<li>求解协方差矩阵 <img src="/2017/10/29/Covariance matrix/AB84F733-5881-4B2C-8FE7-BE2099965DA2.png" alt=""></li>
<li>利用算法求解出特征值和特征向量，取特征值最大的K个特征向量构成矩阵W</li>
<li><img src="/2017/10/29/Covariance matrix/0019B688-8129-4EF2-B87A-3ABAF7228268.png" alt="">Z为新的样本数据</li>
</ul>
<h4 id="2-2-如何理解这一过程的原理"><a href="#2-2-如何理解这一过程的原理" class="headerlink" title="2.2.如何理解这一过程的原理"></a>2.2.如何理解这一过程的原理</h4><p><a href="https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html" target="_blank" rel="external">参考文档</a><a href="https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html" target="_blank" rel="external">https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html</a></p>
<p>降低维度其实主要通过两个角度：</p>
<p>1.降噪，使得维度间的相关性尽可能小</p>
<p>2.冗余，就是去掉一些多余的维度，假设在某一维度上面，值都一样那么这样的维度没有任何意义，成为冗余，所以我们尽量使得维度本身的方差大</p>
<p>在了解了降低维度的两个入手点，接下来我们从协方差矩阵自身的特点来看为什么它能够做到这些事情。</p>
<p>从上述协方差S的公式中我们可以看出</p>
<ul>
<li>S矩阵的主对角线上面是表示的维度本身的方差,主对角线上面的值对应了维度的能量，这个对应了去冗余这步</li>
<li>其余位置表示的是两不同维度间的相关性，这可以从<strong>1</strong>中向量积看出，在这些位置中的值要尽可能小，达到降噪的目的</li>
</ul>
<p>所以要解决上述问题，我们需要找个一矩阵P，使得A经过P矩阵的变化后，S主对角线上面的值尽可能大，非主对角线上面的值为0 ,即为对角矩阵<img src="/2017/10/29/Covariance matrix/66350F53-03C4-475F-B4B6-11E97AE8F471.png" alt=""></p>
<p><img src="/2017/10/29/Covariance matrix/F9CF233E-F1FA-421E-AB0F-612252E3A906.png" alt=""></p>
<p>因为协方差矩阵S是实对称矩阵，所以其特征值分解有如下性质</p>
<p><img src="/2017/10/29/Covariance matrix/5583E423-CA73-43B5-AA42-E5F9A1967EC9.png" alt=""></p>
<p>Q为正交矩阵满足 <img src="/2017/10/29/Covariance matrix/9A78E778-4C01-4AE1-9411-E461D3DB9E39.png" alt=""></p>
<p>所以对S进行分解即可以求解P</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-algebra/">Linear algebra</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCA/">PCA</a></li></ul>

      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/11/04/统计学方法-李航-chapter-one/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          统计学方法-chapter one
        
      </div>
    </a>
  
  
</nav>

  
</article>




<div class="share_addthis">
  <div class="sharing addthis_toolbox share">
    <a class="addthis_button_facebook_like"></a>
    <a class="addthis_button_tweet"></a>
    <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-560c64c35486b3d4" async="async"></script>
</div>





</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Meixinyu&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<!--page counter part-->
<script>
function addCount (Counter) {
        url=$('.article-date').attr('href').trim();
    title = $('.article-title').text().trim();
    var query=new AV.Query(Counter);
    //use url as unique idnetfication
    query.equalTo("url",url);
    query.find({
        success: function(results){
            if(results.length>0)
            {
                var counter=results[0];
                counter.fetchWhenSave(true); //get recent result
                counter.increment("time");
                counter.save();
            }
            else
            {
                var newcounter=new Counter();
                newcounter.set("title",title);
                newcounter.set("url",url);
                newcounter.set("time",1);
                newcounter.save(null,{
                    success: function(newcounter){
                    //alert('New object created');
                    },
                    error: function(newcounter,error){
                    alert('Failed to create');
                    }
                    });
            }
        },
        error: function(error){
            //find null is not a error
            alert('Error:'+error.code+" "+error.message);
        }
    });
}
$(function(){
    var Counter=AV.Object.extend("Counter");
    //only increse visit counting when intering a page
    if ($('.article-title').length == 1)
       addCount(Counter);
    var query=new AV.Query(Counter);
    query.descending("time");
    // the sum of popular posts
    query.limit(10); 
    query.find({
        success: function(results){
                for(var i=0;i<results.length;i++)    
                {
                    var counter=results[i];
                    title=counter.get("title");
                    url=counter.get("url");
                    time=counter.get("time");
                    // add to the popularlist widget
                    showcontent=title+" ("+time+")";
                    //notice the "" in href
                    $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                }
            },
        error: function(error){
            alert("Error:"+error.code+" "+error.message);
        }
        }
    )
    });
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>