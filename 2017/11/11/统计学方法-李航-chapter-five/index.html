<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>统计学方法-chapter five - Meixiuxiu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这一章的内容比较新吧，之前在西瓜书上面看过决策树，但在这章还是有很多新东西，所以要好好总结下。同时也温习下内容。 每次写博客最不喜欢写公式了，看了几个博客也没能让hexo完美支持数学公式，所以写了公式然后截图，博客还很简陋，有时间再搭下，加点功能… 1.1 特征选择在每一个非叶节点都是针对特征的一个判断，那么在每个非叶节点的特征选择就成为了一个需要考虑的问题。通常特征选择的准则是信息增益或者信息增">
<meta name="keywords" content="决策树">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学方法-chapter five">
<meta property="og:url" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/index.html">
<meta property="og:site_name" content="Meixiuxiu">
<meta property="og:description" content="这一章的内容比较新吧，之前在西瓜书上面看过决策树，但在这章还是有很多新东西，所以要好好总结下。同时也温习下内容。 每次写博客最不喜欢写公式了，看了几个博客也没能让hexo完美支持数学公式，所以写了公式然后截图，博客还很简陋，有时间再搭下，加点功能… 1.1 特征选择在每一个非叶节点都是针对特征的一个判断，那么在每个非叶节点的特征选择就成为了一个需要考虑的问题。通常特征选择的准则是信息增益或者信息增">
<meta property="og:locale" content="zh-tw">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/7.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/8.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/9.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/10.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/11.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/1.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/2.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/12.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/13.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/14.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/15.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/3.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/4.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/5.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/16.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/17.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/18.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/19.png">
<meta property="og:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/6.png">
<meta property="og:updated_time" content="2017-11-11T12:05:12.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计学方法-chapter five">
<meta name="twitter:description" content="这一章的内容比较新吧，之前在西瓜书上面看过决策树，但在这章还是有很多新东西，所以要好好总结下。同时也温习下内容。 每次写博客最不喜欢写公式了，看了几个博客也没能让hexo完美支持数学公式，所以写了公式然后截图，博客还很简陋，有时间再搭下，加点功能… 1.1 特征选择在每一个非叶节点都是针对特征的一个判断，那么在每个非叶节点的特征选择就成为了一个需要考虑的问题。通常特征选择的准则是信息增益或者信息增">
<meta name="twitter:image" content="http://yoursite.com/2017/11/11/统计学方法-李航-chapter-five/7.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer"><article id="post-统计学方法-李航-chapter-five" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      统计学方法-chapter five
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/11/11/统计学方法-李航-chapter-five/" class="article-date">
  <time datetime="2017-11-11T10:04:16.000Z" itemprop="datePublished">2017-11-11</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/统计学习方法-李航/">统计学习方法(李航)</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>这一章的内容比较新吧，之前在西瓜书上面看过决策树，但在这章还是有很多新东西，所以要好好总结下。同时也温习下内容。</p>
<p>每次写博客最不喜欢写公式了，看了几个博客也没能让hexo完美支持数学公式，所以写了公式然后截图，博客还很简陋，有时间再搭下，加点功能…</p>
<h3 id="1-1-特征选择"><a href="#1-1-特征选择" class="headerlink" title="1.1 特征选择"></a>1.1 特征选择</h3><p>在每一个非叶节点都是针对特征的一个判断，那么在每个非叶节点的特征选择就成为了一个需要考虑的问题。通常特征选择的准则是信息增益或者信息增益比。下面介绍信息增益和信息增益比。<br>在介绍信息增益和信息增益比之前需要介绍下熵和条件熵，熵(entropy)是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/7.png" alt=""></p>
<p>则随机变量X的熵(entropy)定义为,单位通常为bit(nat)：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/8.png" alt=""></p>
<p>条件熵H(Y|X),表示为在X确定的情况下随机变量Y的不确定性(熵)，N为X的取值个数，pi表示当前值的概率,通常采用极大似然法进行估计:<br><img src="/2017/11/11/统计学方法-李航-chapter-five/9.png" alt=""></p>
<p>①信息增益：现在给出信息增益(information gain)的定义,信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度,公式如下，D为数据集，A为特征：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/10.png" alt=""></p>
<p>②信息增益比: 公式如下：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/11.png" alt="11"></p>
<p>从而我们可以知道每次特征的选取，我们就选取使得信息增益或者是信息增益比最大的特征。</p>
<h3 id="1-2-决策树的生成算法"><a href="#1-2-决策树的生成算法" class="headerlink" title="1.2 决策树的生成算法"></a>1.2 决策树的生成算法</h3><p>其实知道如何选取特征之后，决策树的生成其实就很简单了，截图书中的算法</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/1.png" alt=""></p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/2.png" alt=""></p>
<p>只是在判断叶子节点的类别时，需要注意下叶子节点的类别是叶子节点最多的类，如果把信息增益换成是信息增益比那么久是C4.5算法</p>
<h3 id="1-3-决策树的剪枝"><a href="#1-3-决策树的剪枝" class="headerlink" title="1.3 决策树的剪枝"></a>1.3 决策树的剪枝</h3><p>决策树有一个很明显的问题就是可能存在过拟合的情况，过拟合就是树太大了，之前在看过的资料中都是通过加上regularization项，其实这里和之前看的都差不多思想。避免过拟合问题主要通过树的剪枝实现，这个剪枝和acm不太一样呢，<del>~~(&gt;_&lt;)</del>~~<br>为避免过拟合加上的regularization项的loss(cost) function为：<br>其中树T的叶节点个数为|T|,t是树的叶节点，叶节点有Nt个样本点, k(范围为1~K)类的样本点有Ntk，α为一个自定义常数参数</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/12.png" alt=""></p>
<p>利用极大似然估计我们给出叶节点t上的熵为:</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/13.png" alt=""></p>
<p>这样我们决定一个子树要不要被去掉那么，就通过比较去掉前后的cost function来决定要不要剪枝，利用递归好像就可以实现，可以通过动态规划的方法实现局部进行，没有深究，如果以后以后要写的话，再来好好看看要怎么弄吧。</p>
<h3 id="1-4-CART-classification-and-regression-tree-算法"><a href="#1-4-CART-classification-and-regression-tree-算法" class="headerlink" title="1.4 CART(classification and regression tree)算法"></a>1.4 CART(classification and regression tree)算法</h3><h4 id="1-4-1-CART的生成"><a href="#1-4-1-CART的生成" class="headerlink" title="1.4.1 CART的生成"></a>1.4.1 CART的生成</h4><p>1.回归树的生成<br>假设X和Y分别为输入和输出变量，Y是连续变量，给定训练集:</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/14.png" alt=""></p>
<p>对于一个回归树的特征空间的每一个被划分的单元，我们假设已经将特征空间分为M个单元，R1….Rm….RM，每个单元的输出值为cm，于是我们将回归树模型表示为：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/15.png" alt=""></p>
<p>如何选择最优特征和值，不想打公式了，截个图</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/3.png" alt=""></p>
<p>2.分类树的生成<br>其实分类树的生成和ID13算法差不多，只是不采用信息增益的方法而是采用了基尼指数的方法去选取特征。<br>定义基尼指数：(又要截图了。。。。)<br><img src="/2017/11/11/统计学方法-李航-chapter-five/4.png" alt=""><br><img src="/2017/11/11/统计学方法-李航-chapter-five/5.png" alt=""><br>至于生成树的算法其实也和上面的差不多。</p>
<h4 id="1-4-2-CART剪枝"><a href="#1-4-2-CART剪枝" class="headerlink" title="1.4.2 CART剪枝"></a>1.4.2 CART剪枝</h4><p>个人感觉剪枝这部分还是挺有意思的，其实就是在每一次剪枝的过程中都会生成一个新的树(形成子树序列)，为了去从这其中选取合适的书，采用交叉验证法在独立的验证数据集上面进行测试，感觉CART比前面的ID13会好很多，因为没有在新的数据集上面进行数据的验证感觉还是不行。验证集必须是单独的部分。<br>在剪枝过程中，我们定义子树(子树序列中的一个子树)的cost function为：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/16.png" alt=""></p>
<p>这里生成的子树序列其实是一整棵树，不是字面上的子树，T0为初始的树，对T0的任意内部节点t,以节点t为单节点树的cost function是：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/17.png" alt=""></p>
<p>以t为根节点的子树Tt的cost function是：</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/18.png" alt=""></p>
<p>这个部分比较有意思，随着α的增大我们来看看会有什么事情发生</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/19.png" alt=""></p>
<p>Tt和t有相同的cost function值，而t的结点减少，因此t比Tt更可取，可以进行剪枝。所以对每一内部结点t计算g(t)，g(t)表示了剪枝后整体cost function减少的程度，在T0中减去最小g(t)的Tt，得到的子树为T1,同时将最小的g(t)设置为α1，T为区间[α1，α2)的最优子树</p>
<p><img src="/2017/11/11/统计学方法-李航-chapter-five/6.png" alt=""></p>
<h3 id="1-5习题的讨论"><a href="#1-5习题的讨论" class="headerlink" title="1.5习题的讨论"></a>1.5习题的讨论</h3><p>5.3其实很简单，α确定时我们可以固定的找到α对应的区间，然后可以找到相应的子树，这个子树一定是最优的，因为对应前一个的区间树，还有可以剪枝的点，对应后一个区间的树，又多剪枝了点</p>
<p>5.4类似5.3</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/决策树/">决策树</a></li></ul>

      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/11/10/统计学方法-李航-chapter-four/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">统计学方法-chapter four&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>




<div class="share_addthis">
  <div class="sharing addthis_toolbox share">
    <a class="addthis_button_facebook_like"></a>
    <a class="addthis_button_tweet"></a>
    <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-560c64c35486b3d4" async="async"></script>
</div>





</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Meixinyu&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>