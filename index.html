<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Meixiuxiu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Meixiuxiu">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Meixiuxiu">
<meta property="og:locale" content="zh-tw">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Meixiuxiu">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer">
      <article id="post-统计学方法-李航-chapter-two" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/11/06/统计学方法-李航-chapter-two/">统计学方法-chapter two</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/11/06/统计学方法-李航-chapter-two/" class="article-date">
  <time datetime="2017-11-06T11:37:52.000Z" itemprop="datePublished">2017-11-06</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/统计学习方法-李航/">统计学习方法(李航)</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/2017/11/06/统计学方法-李航-chapter-two/faker.png" alt="skt-faker"></p>
<p>一些最近感想</p>
<p>今年是我以第一次接触lol,虽然我不玩这个游戏，但是我觉得总决赛看起来还是很精彩，很感动。决赛是ssg vs skt, 霸主skt输了，最后skt中单faker哭了。全场看下来，skt基本上faker一个人carry,  赛后采访ssg的选手也说自己一直很努力地在训练。后来了解了下这个选手，真的是要努力有努力，要天赋有天赋。看得出来电子竞技选手都是很辛苦的，都是需要付出，哪怕再有天赋，没有足够后天的努力也是瞎话。付出够多常胜将军也有被你打败的一天，但若你松懈下来，后面的就会追赶上来。</p>
<p>哎，想想自己，保研后过的像个咸鱼一样，是不是不该这么松懈？至少行动要配得上梦想。</p>
<p>第二章课后习题比较简单，主要做下关于perceptron 算法的summary </p>
<p>let’s start.</p>
<h3 id="1-1-Perceptron"><a href="#1-1-Perceptron" class="headerlink" title="1.1 Perceptron"></a>1.1 Perceptron</h3><p><img src="/2017/11/06/统计学方法-李航-chapter-two/1.png" alt=""></p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/2.png" alt=""></p>
<p>感知学习的目标就是能够找到一个将训练集正实例点和负实例点分开的超平面。<br>感知学习的costFunction (损失函数)定义为：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/5.png" alt=""></p>
<h3 id="1-2-Perceptron-learning-algorithm"><a href="#1-2-Perceptron-learning-algorithm" class="headerlink" title="1.2 Perceptron learning algorithm"></a>1.2 Perceptron learning algorithm</h3><h4 id="1-2-1原始学习算法"><a href="#1-2-1原始学习算法" class="headerlink" title="1.2.1原始学习算法"></a>1.2.1原始学习算法</h4><p>因为costFunction是凹函数，我们具体采用stochastic gradient descent算法<br>求导得梯度函数：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/3.png" alt=""></p>
<p>关于算法的收敛性:P31</p>
<h4 id="1-2-2对偶学习算法"><a href="#1-2-2对偶学习算法" class="headerlink" title="1.2.2对偶学习算法"></a>1.2.2对偶学习算法</h4><p>假设w,b初始值都初始化为0，那么最后的w,b值可以写为：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/4.png" alt=""></p>
<p>那么函数就为：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/6.png" alt=""></p>
<p>对偶学习算法只是函数值改变了，算法过程还是和原始学习算法一样</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Perceptron/">Perceptron</a></li></ul>

      </footer>
    
  </div>
  
</article>








    
      <article id="post-统计学方法-李航-chapter-one" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/11/04/统计学方法-李航-chapter-one/">统计学方法-chapter one</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/11/04/统计学方法-李航-chapter-one/" class="article-date">
  <time datetime="2017-11-04T06:28:14.000Z" itemprop="datePublished">2017-11-04</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/统计学习方法-李航/">统计学习方法(李航)</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/2017/11/04/统计学方法-李航-chapter-one/食堂旁边的猫猫.jpg" alt="食堂旁边的猫猫"></p>
<p>因为第一章比较简单，所以内容上面没什么好总结的，主要写下课后习题</p>
<p>看见猫猫就开心啊</p>
<h3 id="1-1极大似然估计"><a href="#1-1极大似然估计" class="headerlink" title="1.1极大似然估计"></a>1.1极大似然估计</h3><p>极大似然估计是用给定的数据来估计模型参数的方法</p>
<p><a href="http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html" target="_blank" rel="external">参考文档</a><a href="http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html" target="_blank" rel="external">http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html</a></p>
<p>利用极大似然估计结果为1的概率</p>
<p>令P为结果为1的概率则<img src="/2017/11/04/统计学方法-李航-chapter-one/1.png" alt=""></p>
<p>求满足(1)式的最大值，求得P = k/n</p>
<h3 id="1-2-贝叶斯估计"><a href="#1-2-贝叶斯估计" class="headerlink" title="1.2 贝叶斯估计"></a>1.2 贝叶斯估计</h3><p>P服从B分布</p>
<p><img src="/2017/11/04/统计学方法-李航-chapter-one/B分布概率密度函数.jpg" alt=""></p>
<p>来自wiki截图<a href="https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83" target="_blank" rel="external">https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83</a></p>
<p>则求下式最大值</p>
<p><img src="/2017/11/04/统计学方法-李航-chapter-one/2.png" alt=""></p>
<p>P  = (k+α-1)/(n+α-1+β-1)</p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>第二道题很简单,能把1.1中的参考文档看了就行了</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/极大似然估计/">极大似然估计</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/贝叶斯估计/">贝叶斯估计</a></li></ul>

      </footer>
    
  </div>
  
</article>








    
      <article id="post-Covariance matrix" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/29/Covariance matrix/">浅谈PCA中的Covariance matrix</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/10/29/Covariance matrix/" class="article-date">
  <time datetime="2017-10-29T10:46:01.000Z" itemprop="datePublished">2017-10-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a><span>></span><a class="article-category-link" href="/categories/machine-learning/斯坦福机器学习公开课/">斯坦福机器学习公开课</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><img src="/2017/10/29/Covariance matrix/楼下的猫猫.JPG" alt="楼下的猫猫"></p>
<h3 id="1-向量乘积的几何意义"><a href="#1-向量乘积的几何意义" class="headerlink" title="1.向量乘积的几何意义"></a>1.向量乘积的几何意义</h3><p>每次都要忘记这些东西所以记下来==</p>
<p>对于向量 <strong>a</strong> 和 <strong>b</strong> ,向量乘积定义为:<img src="/2017/10/29/Covariance matrix/27A466BF-B714-4F71-9718-D892C69191CA.png" alt=""></p>
<p>向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积</p>
<p>描述了向量间的相关性</p>
<h3 id="2-PCA中的协方差矩阵"><a href="#2-PCA中的协方差矩阵" class="headerlink" title="2.PCA中的协方差矩阵"></a>2.PCA中的协方差矩阵</h3><h4 id="2-1-PCA算法"><a href="#2-1-PCA算法" class="headerlink" title="2.1.PCA算法"></a>2.1.PCA算法</h4><p>PCA算法是一种数据降维的算法，通过某种线性投影将高维的数据映射到低维的空间中，以此用比较少的数据维度保留住原数据的特性。</p>
<p>假设令A为m*n的原数据 ，m为样本数量，n为维度</p>
<ul>
<li>对每个维度的数据进行中心化</li>
<li>求解协方差矩阵 <img src="/2017/10/29/Covariance matrix/AB84F733-5881-4B2C-8FE7-BE2099965DA2.png" alt=""></li>
<li>利用算法求解出特征值和特征向量，取特征值最大的K个特征向量构成矩阵W</li>
<li><img src="/2017/10/29/Covariance matrix/0019B688-8129-4EF2-B87A-3ABAF7228268.png" alt="">Z为新的样本数据</li>
</ul>
<h4 id="2-2-如何理解这一过程的原理"><a href="#2-2-如何理解这一过程的原理" class="headerlink" title="2.2.如何理解这一过程的原理"></a>2.2.如何理解这一过程的原理</h4><p><a href="https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html" target="_blank" rel="external">参考文档</a><a href="https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html" target="_blank" rel="external">https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html</a></p>
<p>降低维度其实主要通过两个角度：</p>
<p>1.降噪，使得维度间的相关性尽可能小</p>
<p>2.冗余，就是去掉一些多余的维度，假设在某一维度上面，值都一样那么这样的维度没有任何意义，成为冗余，所以我们尽量使得维度本身的方差大</p>
<p>在了解了降低维度的两个入手点，接下来我们从协方差矩阵自身的特点来看为什么它能够做到这些事情。</p>
<p>从上述协方差S的公式中我们可以看出</p>
<ul>
<li>S矩阵的主对角线上面是表示的维度本身的方差,主对角线上面的值对应了维度的能量，这个对应了去冗余这步</li>
<li>其余位置表示的是两不同维度间的相关性，这可以从<strong>1</strong>中向量积看出，在这些位置中的值要尽可能小，达到降噪的目的</li>
</ul>
<p>所以要解决上述问题，我们需要找个一矩阵P，使得A经过P矩阵的变化后，S主对角线上面的值尽可能大，非主对角线上面的值为0 ,即为对角矩阵<img src="/2017/10/29/Covariance matrix/66350F53-03C4-475F-B4B6-11E97AE8F471.png" alt=""></p>
<p><img src="/2017/10/29/Covariance matrix/F9CF233E-F1FA-421E-AB0F-612252E3A906.png" alt=""></p>
<p>因为协方差矩阵S是实对称矩阵，所以其特征值分解有如下性质</p>
<p><img src="/2017/10/29/Covariance matrix/5583E423-CA73-43B5-AA42-E5F9A1967EC9.png" alt=""></p>
<p>Q为正交矩阵满足 <img src="/2017/10/29/Covariance matrix/9A78E778-4C01-4AE1-9411-E461D3DB9E39.png" alt=""></p>
<p>所以对S进行分解即可以求解P</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-algebra/">Linear algebra</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCA/">PCA</a></li></ul>

      </footer>
    
  </div>
  
</article>








    </section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Meixinyu&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>