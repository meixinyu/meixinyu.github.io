<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Meixiuxiu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Meixiuxiu">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Meixiuxiu">
<meta property="og:locale" content="zh-tw">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Meixiuxiu">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer">
      <article id="post-统计学方法-李航-chapter-four" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/11/10/统计学方法-李航-chapter-four/">统计学方法-chapter four</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/11/10/统计学方法-李航-chapter-four/" class="article-date">
  <time datetime="2017-11-10T03:28:18.000Z" itemprop="datePublished">2017-11-10</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/统计学习方法-李航/">统计学习方法(李航)</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>这一章有点容易让人迷糊，所以打算详细写下</p>
<h3 id="1-1-朴素贝叶斯的基本公式"><a href="#1-1-朴素贝叶斯的基本公式" class="headerlink" title="1.1 朴素贝叶斯的基本公式"></a>1.1 朴素贝叶斯的基本公式</h3><ul>
<li><p>X是定义在输入空间的n维的随机向量,Y是定义在输出空间的随机变量,现在样本数量为N的样本集。由P(X,Y)独立同分布产生。朴素贝叶斯通过样本集训练数据学习联合概率分布P(X,Y)。</p>
</li>
<li><p>先验概率分布，条件概率分布分别为<br><img src="/2017/11/10/统计学方法-李航-chapter-four/1.png" alt=""></p>
</li>
<li><p>朴素贝叶斯假设条件独立性，等于说是用于分类的特征 (即每一维)在类确定的条件下都是条件独立的,那么条件概率分布可以写为：</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/2.png" alt=""></p>
<p>接下来进行朴素贝叶斯分类的公式推导，计算后验概率分布：</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/3.png" alt=""></p>
<p>将后验概率值最大的类作为x的类输出，根据贝叶斯定理,后验概率可以写为：</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/7.png" alt=""></p>
<p>将条件独立时的条件概率分布公式带入后验概率中可以得到</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/8.png" alt=""></p>
<p>可以看出分母对所有的Y的情况都是相同的，那么我们就是要求得Ck的值使得（2）中的分子值最大</p>
</li>
</ul>
<h3 id="1-2-为什么要后验概率最大化来求得x的类输出"><a href="#1-2-为什么要后验概率最大化来求得x的类输出" class="headerlink" title="1.2 为什么要后验概率最大化来求得x的类输出"></a>1.2 为什么要后验概率最大化来求得x的类输出</h3><p>我们要求得x的类，使得期望风险最小化，截图书中的证明<img src="/2017/11/10/统计学方法-李航-chapter-four/4.png" alt=""></p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/5.png" alt=""></p>
<h3 id="1-3-朴素贝叶斯的参数估计"><a href="#1-3-朴素贝叶斯的参数估计" class="headerlink" title="1.3 朴素贝叶斯的参数估计"></a>1.3 朴素贝叶斯的参数估计</h3><p>我们已经求出来朴素贝叶斯的公式，那么如何对(2)式中的分子求值然后进行比较呢？参数估计采用了极大似然的方法，首先对P(Y = Ck)进行极大似然估计，在之前就有了解过极大似然法求值，那么对于这样的多分类问题说，其实方法差不多，对于每一个类我们假设概率为p，那么当前类的概率为p,其他类的概率为(1-p)，这样同理之前的问题，就转化为当前类求最大值，求导即可以求得最大值，这样就可以结局4.1的习题。先验概率的极大似然估计为：</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/9.png" alt=""></p>
<p>同理可以得到条件概率的极大似然估计是 (其中第j个特征，可能的取值的集合为ajt, t在为第j个特征的取值个数)：</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/10.png" alt=""></p>
<h3 id="1-4-避免概率值为0的情况"><a href="#1-4-避免概率值为0的情况" class="headerlink" title="1.4 避免概率值为0的情况"></a>1.4 避免概率值为0的情况</h3><p>极大似然法可能出现概率值为0的情况，其实这个情况我也看的不是很懂呐，如果以后遇见了弄明白了再来好好写写，先把书中的公式截图过来，课后习题4.2的第一问，看了一个博客差不多懂了，第二个搞不明白。。。，所以还是以后会了再来一起写吧。</p>
<p><img src="/2017/11/10/统计学方法-李航-chapter-four/6.png" alt=""></p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/朴素贝叶斯/">朴素贝叶斯</a></li></ul>

      </footer>
    
  </div>
  
</article>








    
      <article id="post-统计学方法-李航-chapter-two" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/11/06/统计学方法-李航-chapter-two/">统计学方法-chapter two</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/11/06/统计学方法-李航-chapter-two/" class="article-date">
  <time datetime="2017-11-06T11:37:52.000Z" itemprop="datePublished">2017-11-06</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/统计学习方法-李航/">统计学习方法(李航)</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/2017/11/06/统计学方法-李航-chapter-two/faker.png" alt="skt-faker"></p>
<p>一些最近感想</p>
<p>今年是我以第一次接触lol,虽然我不玩这个游戏，但是我觉得总决赛看起来还是很精彩，很感动。决赛是ssg vs skt, 霸主skt输了，最后skt中单faker哭了。全场看下来，skt基本上faker一个人carry,  赛后采访ssg的选手也说自己一直很努力地在训练。后来了解了下这个选手，真的是要努力有努力，要天赋有天赋。看得出来电子竞技选手都是很辛苦的，都是需要付出，哪怕再有天赋，没有足够后天的努力也是瞎话。付出够多常胜将军也有被你打败的一天，但若你松懈下来，后面的就会追赶上来。</p>
<p>哎，想想自己，保研后过的像个咸鱼一样，是不是不该这么松懈？至少行动要配得上梦想。</p>
<p>第二章课后习题比较简单，主要做下关于perceptron 算法的summary </p>
<p>let’s start.</p>
<h3 id="1-1-Perceptron"><a href="#1-1-Perceptron" class="headerlink" title="1.1 Perceptron"></a>1.1 Perceptron</h3><p><img src="/2017/11/06/统计学方法-李航-chapter-two/1.png" alt=""></p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/2.png" alt=""></p>
<p>感知学习的目标就是能够找到一个将训练集正实例点和负实例点分开的超平面。<br>感知学习的costFunction (损失函数)定义为：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/5.png" alt=""></p>
<h3 id="1-2-Perceptron-learning-algorithm"><a href="#1-2-Perceptron-learning-algorithm" class="headerlink" title="1.2 Perceptron learning algorithm"></a>1.2 Perceptron learning algorithm</h3><h4 id="1-2-1原始学习算法"><a href="#1-2-1原始学习算法" class="headerlink" title="1.2.1原始学习算法"></a>1.2.1原始学习算法</h4><p>因为costFunction是凹函数，我们具体采用stochastic gradient descent算法<br>求导得梯度函数：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/3.png" alt=""></p>
<p>关于算法的收敛性:P31</p>
<h4 id="1-2-2对偶学习算法"><a href="#1-2-2对偶学习算法" class="headerlink" title="1.2.2对偶学习算法"></a>1.2.2对偶学习算法</h4><p>假设w,b初始值都初始化为0，那么最后的w,b值可以写为：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/4.png" alt=""></p>
<p>那么函数就为：</p>
<p><img src="/2017/11/06/统计学方法-李航-chapter-two/6.png" alt=""></p>
<p>对偶学习算法只是函数值改变了，算法过程还是和原始学习算法一样</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Perceptron/">Perceptron</a></li></ul>

      </footer>
    
  </div>
  
</article>








    
      <article id="post-统计学方法-李航-chapter-one" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/11/04/统计学方法-李航-chapter-one/">统计学方法-chapter one</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/11/04/统计学方法-李航-chapter-one/" class="article-date">
  <time datetime="2017-11-04T06:28:14.000Z" itemprop="datePublished">2017-11-04</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/统计学习方法-李航/">统计学习方法(李航)</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/2017/11/04/统计学方法-李航-chapter-one/食堂旁边的猫猫.jpg" alt="食堂旁边的猫猫"></p>
<p>因为第一章比较简单，所以内容上面没什么好总结的，主要写下课后习题</p>
<p>看见猫猫就开心啊</p>
<h3 id="1-1极大似然估计"><a href="#1-1极大似然估计" class="headerlink" title="1.1极大似然估计"></a>1.1极大似然估计</h3><p>极大似然估计是用给定的数据来估计模型参数的方法</p>
<p><a href="http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html" target="_blank" rel="external">参考文档</a><a href="http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html" target="_blank" rel="external">http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html</a></p>
<p>利用极大似然估计结果为1的概率</p>
<p>令P为结果为1的概率则<img src="/2017/11/04/统计学方法-李航-chapter-one/1.png" alt=""></p>
<p>求满足(1)式的最大值，求得P = k/n</p>
<h3 id="1-2-贝叶斯估计"><a href="#1-2-贝叶斯估计" class="headerlink" title="1.2 贝叶斯估计"></a>1.2 贝叶斯估计</h3><p>P服从B分布</p>
<p><img src="/2017/11/04/统计学方法-李航-chapter-one/B分布概率密度函数.jpg" alt=""></p>
<p>来自wiki截图<a href="https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83" target="_blank" rel="external">https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83</a></p>
<p>则求下式最大值</p>
<p><img src="/2017/11/04/统计学方法-李航-chapter-one/2.png" alt=""></p>
<p>P  = (k+α-1)/(n+α-1+β-1)</p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>第二道题很简单,能把1.1中的参考文档看了就行了</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/极大似然估计/">极大似然估计</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/贝叶斯估计/">贝叶斯估计</a></li></ul>

      </footer>
    
  </div>
  
</article>








    
      <article id="post-Covariance matrix" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/29/Covariance matrix/">浅谈PCA中的Covariance matrix</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/10/29/Covariance matrix/" class="article-date">
  <time datetime="2017-10-29T10:46:01.000Z" itemprop="datePublished">2017-10-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a><span>></span><a class="article-category-link" href="/categories/machine-learning/斯坦福机器学习公开课/">斯坦福机器学习公开课</a>
  </div>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><img src="/2017/10/29/Covariance matrix/楼下的猫猫.JPG" alt="楼下的猫猫"></p>
<h3 id="1-向量乘积的几何意义"><a href="#1-向量乘积的几何意义" class="headerlink" title="1.向量乘积的几何意义"></a>1.向量乘积的几何意义</h3><p>每次都要忘记这些东西所以记下来==</p>
<p>对于向量 <strong>a</strong> 和 <strong>b</strong> ,向量乘积定义为:<img src="/2017/10/29/Covariance matrix/27A466BF-B714-4F71-9718-D892C69191CA.png" alt=""></p>
<p>向量内积的几何解释就是一个向量在另一个向量上的投影的积，也就是同方向的积</p>
<p>描述了向量间的相关性</p>
<h3 id="2-PCA中的协方差矩阵"><a href="#2-PCA中的协方差矩阵" class="headerlink" title="2.PCA中的协方差矩阵"></a>2.PCA中的协方差矩阵</h3><h4 id="2-1-PCA算法"><a href="#2-1-PCA算法" class="headerlink" title="2.1.PCA算法"></a>2.1.PCA算法</h4><p>PCA算法是一种数据降维的算法，通过某种线性投影将高维的数据映射到低维的空间中，以此用比较少的数据维度保留住原数据的特性。</p>
<p>假设令A为m*n的原数据 ，m为样本数量，n为维度</p>
<ul>
<li>对每个维度的数据进行中心化</li>
<li>求解协方差矩阵 <img src="/2017/10/29/Covariance matrix/AB84F733-5881-4B2C-8FE7-BE2099965DA2.png" alt=""></li>
<li>利用算法求解出特征值和特征向量，取特征值最大的K个特征向量构成矩阵W</li>
<li><img src="/2017/10/29/Covariance matrix/0019B688-8129-4EF2-B87A-3ABAF7228268.png" alt="">Z为新的样本数据</li>
</ul>
<h4 id="2-2-如何理解这一过程的原理"><a href="#2-2-如何理解这一过程的原理" class="headerlink" title="2.2.如何理解这一过程的原理"></a>2.2.如何理解这一过程的原理</h4><p><a href="https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html" target="_blank" rel="external">参考文档</a><a href="https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html" target="_blank" rel="external">https://wenku.baidu.com/view/ce7ee04bcc175527072208ca.html</a></p>
<p>降低维度其实主要通过两个角度：</p>
<p>1.降噪，使得维度间的相关性尽可能小</p>
<p>2.冗余，就是去掉一些多余的维度，假设在某一维度上面，值都一样那么这样的维度没有任何意义，成为冗余，所以我们尽量使得维度本身的方差大</p>
<p>在了解了降低维度的两个入手点，接下来我们从协方差矩阵自身的特点来看为什么它能够做到这些事情。</p>
<p>从上述协方差S的公式中我们可以看出</p>
<ul>
<li>S矩阵的主对角线上面是表示的维度本身的方差,主对角线上面的值对应了维度的能量，这个对应了去冗余这步</li>
<li>其余位置表示的是两不同维度间的相关性，这可以从<strong>1</strong>中向量积看出，在这些位置中的值要尽可能小，达到降噪的目的</li>
</ul>
<p>所以要解决上述问题，我们需要找个一矩阵P，使得A经过P矩阵的变化后，S主对角线上面的值尽可能大，非主对角线上面的值为0 ,即为对角矩阵<img src="/2017/10/29/Covariance matrix/66350F53-03C4-475F-B4B6-11E97AE8F471.png" alt=""></p>
<p><img src="/2017/10/29/Covariance matrix/F9CF233E-F1FA-421E-AB0F-612252E3A906.png" alt=""></p>
<p>因为协方差矩阵S是实对称矩阵，所以其特征值分解有如下性质</p>
<p><img src="/2017/10/29/Covariance matrix/5583E423-CA73-43B5-AA42-E5F9A1967EC9.png" alt=""></p>
<p>Q为正交矩阵满足 <img src="/2017/10/29/Covariance matrix/9A78E778-4C01-4AE1-9411-E461D3DB9E39.png" alt=""></p>
<p>所以对S进行分解即可以求解P</p>

      
    </div>
    
    
      <footer class="article-footer">
        
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-algebra/">Linear algebra</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCA/">PCA</a></li></ul>

      </footer>
    
  </div>
  
</article>








    </section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Meixinyu&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script src="/js/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>